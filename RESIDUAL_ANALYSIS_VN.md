# BÃ¡o CÃ¡o PhÃ¢n TÃ­ch Model LSTMConv - Residual Connections

## ğŸ¯ Káº¿t Luáº­n ChÃ­nh

### âœ… **Model ÄÃƒ cÃ³ bá»™ Residual**

Model LSTMConv trong repository `lstmconv-flow` **ÄÃƒ ÄÆ¯á»¢C TÃCH Há»¢P Äáº¦Y Äá»¦** cÃ¡c loáº¡i Residual connections khÃ¡c nhau, bao gá»“m:

## ğŸ“Š Chi Tiáº¿t CÃ¡c Loáº¡i Residual Connections

### 1. **Classic Residual Connection - ResNetBlock**
```python
# File: models/model_Griffin/coffin.py
class ResNetBlock(nn.Module):
    def forward(self, x):
        identity = x                    # LÆ°u input gá»‘c
        out = self.conv1(x)            # Xá»­ lÃ½ qua conv layers
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        
        out += identity                 # â† RESIDUAL CONNECTION
        out = self.relu(out)
        return out
```
**Chá»©c nÄƒng**: GiÃºp gradient flow tá»‘t hÆ¡n, trÃ¡nh vanishing gradient problem.

### 2. **Skip Connections - DecodingBlock (U-Net Style)**
```python
# File: models/model_Griffin/blocks_griffin.py
def forward(self, x, res):
    x = self.up_sample(x)              # Upsample decoder features
    x = self.conv2d_1(x) + res         # â† SKIP CONNECTION tá»« encoder
    x = F.relu(self.batch_norm_1(x))
    return x
```
**Chá»©c nÄƒng**: Káº¿t ná»‘i encoder vÃ  decoder, giá»¯ láº¡i thÃ´ng tin chi tiáº¿t tá»« cÃ¡c layer trÆ°á»›c.

### 3. **Residual Attention - AttentionBlock**
```python
# File: models/model_4/blocks_griffin.py
def forward(self, x):
    F_in = x                           # LÆ°u input gá»‘c
    x = self.conv_layers(x)           # Xá»­ lÃ½ qua conv layers
    x = self.ca_block(x)              # Channel attention
    x = self.sa_block(x)              # Spatial attention
    return x + F_in                    # â† RESIDUAL CONNECTION
```
**Chá»©c nÄƒng**: Káº¿t há»£p attention mechanism vá»›i residual learning.

### 4. **Gating Mechanism - LocalAttention (KhÃ´ng pháº£i residual)**
```python
# File: models/model_Griffin/coffin.py
def forward(self, x):
    attention_weights = torch.sigmoid(self.attention(x))
    return x * attention_weights       # Gating, khÃ´ng pháº£i residual addition
```
**Chá»©c nÄƒng**: Attention-based gating, Ä‘iá»u chá»‰nh importance cá»§a features.

## ğŸ—ï¸ Kiáº¿n TrÃºc Tá»•ng Thá»ƒ

### Core Components:

1. **GatedConvLSTM**: 
   - LSTM vá»›i convolutional gates
   - Xá»­ lÃ½ temporal information
   - CÃ³ 4 gates: forget, input, output, cell

2. **LocalAttention**:
   - Spatial attention mechanism
   - Sá»­ dá»¥ng gating (khÃ´ng pháº£i residual)

3. **HybridBlock**:
   - Káº¿t há»£p GatedConvLSTM + LocalAttention
   - Xá»­ lÃ½ cáº£ temporal vÃ  spatial features

4. **EncodingBlock**:
   - Encoder vá»›i LSTM temporal processing
   - LÆ°u residual outputs cho skip connections

5. **DecodingBlock**:
   - Decoder vá»›i skip connections
   - Káº¿t há»£p features tá»« encoder

6. **FlowPredictionNet**:
   - Máº¡ng chÃ­nh dá»± Ä‘oÃ¡n optical flow
   - U-Net architecture vá»›i LSTM temporal processing

## ğŸ”„ Luá»“ng Dá»¯ Liá»‡u (Data Flow)

```
Input (I0, I1) â†’ Stack â†’ (B,T=2,C,H,W)
    â†“
EncodingBlock1 (LSTM temporal processing) â†’ res1
    â†“ (downsample)
EncodingBlock2 (LSTM temporal processing) â†’ res2  
    â†“ (downsample)
EncodingBlock3 (LSTM temporal processing) â†’ res3
    â†“ (bottleneck)
DecodingBlock1 + res3 (skip connection) ğŸ”„
    â†“ (upsample)
DecodingBlock2 + res2 (skip connection) ğŸ”„
    â†“ (upsample)  
DecodingBlock3 + res1 (skip connection) ğŸ”„
    â†“
Final Conv â†’ Optical Flow (F_0_1, F_1_0)
```

## ğŸ“ˆ Temporal Processing trong LSTM

Má»—i EncodingBlock xá»­ lÃ½ temporal sequence:
```
For t=0: x_0, state = LSTM(x[0], None)      # Initialize
For t=1: x_1, state = LSTM(x[1], state)    # Update vá»›i state tá»« t=0
```

## ğŸ¯ Æ¯u Äiá»ƒm Cá»§a Thiáº¿t Káº¿

1. **Multiple Residual Types**: 3 loáº¡i residual khÃ¡c nhau cho cÃ¡c má»¥c Ä‘Ã­ch khÃ¡c nhau
2. **Temporal Processing**: LSTM xá»­ lÃ½ sequence data hiá»‡u quáº£
3. **Spatial Attention**: LocalAttention táº­p trung vÃ o vÃ¹ng quan trá»ng
4. **Skip Connections**: U-Net style giá»¯ láº¡i thÃ´ng tin chi tiáº¿t
5. **Gradient Flow**: Residual connections giÃºp training á»•n Ä‘á»‹nh

## ğŸ“‹ Tá»•ng Káº¿t

| TiÃªu ChÃ­ | Káº¿t Quáº£ |
|----------|---------|
| **CÃ³ Residual Connections?** | âœ… **CÃ“** (3 loáº¡i khÃ¡c nhau) |
| **Classic Residual** | âœ… ResNetBlock |
| **Skip Connections** | âœ… DecodingBlock (U-Net) |
| **Attention Residual** | âœ… AttentionBlock |
| **Temporal Processing** | âœ… GatedConvLSTM |
| **Spatial Attention** | âœ… LocalAttention |

### ğŸš€ **Káº¿t luáº­n**: Model LSTMConv Ä‘Ã£ Ä‘Æ°á»£c thiáº¿t káº¿ ráº¥t tá»‘t vá»›i Ä‘áº§y Ä‘á»§ cÃ¡c loáº¡i residual connections cáº§n thiáº¿t cho viá»‡c training deep networks vÃ  xá»­ lÃ½ optical flow.

## ğŸ“ Files Tham Kháº£o

- `models/model_Griffin/coffin.py`: GatedConvLSTM, LocalAttention, ResNetBlock
- `models/model_Griffin/hybrid.py`: HybridBlock
- `models/model_Griffin/blocks_griffin.py`: EncodingBlock, DecodingBlock
- `models/model_Griffin/flowpred2_griffin.py`: FlowPredictionNet chÃ­nh
- `models/model_4/blocks_griffin.py`: AttentionBlock vá»›i residual attention

## ğŸ“Š Mermaid Diagrams

1. `model_architecture.mmd`: Kiáº¿n trÃºc tá»•ng thá»ƒ model
2. `data_flow.mmd`: Luá»“ng dá»¯ liá»‡u chi tiáº¿t
3. `component_relationships.mmd`: Má»‘i quan há»‡ giá»¯a cÃ¡c components
4. `residual_analysis.mmd`: PhÃ¢n tÃ­ch residual connections